# **SWAHILI SCAM DETECTION PROJECT WORKFLOW AND Q\&A GUIDE**

This document explains how the project works end-to-end. All claims are grounded in specific files and functions in this project.



## **TERMINOLOGY AND SYNONYMS**



* **Dataset/Data**: Collection of Swahili messages labeled as scam (1) or legitimate (0)
* **Preprocessing**: Text cleaning and normalization
* **Feature Engineering/Feature Extraction**: Converting text into numerical patterns the computer can analyze
* **TF-IDF**: Term Frequency-Inverse Document Frequency (measures word importance)
* **N-grams**: Word sequences (1-gram = single word, 2-gram = word pairs)
* **BiLSTM**: Bidirectional Long Short-Term Memory (a type of neural network)
* **BERT**: Bidirectional Encoder Representations from Transformers (advanced language model)
* **Tokenization**: Breaking text into individual words or subwords
* **Vocabulary/Vocab**: Dictionary of all known words in the dataset
* **Model Training**: Teaching the computer to recognize scam patterns
* **Evaluation**: Testing how well the model performs on new messages
* **API**: Application Programming Interface (web service for predictions)
* **Confidence Score**: How certain the model is about its prediction (0-100%)
* **Traditional ML**: Classical machine learning methods (Random Forest, SVM, etc.)
* **Neural Networks**: AI models inspired by how the brain processes information



### **1. END-TO-END WORKFLOW (WHAT HAPPENS FROM DATA TO SERVING)**

#### **Data Source**

**File**: `data/raw/swahili_messages_sample.csv`

**Generated by**: `data/sample_data.py`

* `create_sample_dataset()` builds a labeled dataset (scam=1, legitimate=0) with categories and metadata
* `save_dataset(df)` writes to `data/raw/swahili_messages_sample.csv` (UTF-8)







#### **Preprocessing** (applies across traditional ML and neural models)

**File**: `src/swahili_preprocessor.py` (class `SwahiliPreprocessor`)

* `clean_text()`: lowercase; removes URLs, emails, phone numbers; normalizes whitespace
* `normalize_slang()`: maps slang/variants to canonical terms using `slang_dict`
* `remove_stopwords()`: removes curated Swahili stopwords (`swahili_stopwords`)
* `extract_basic_features()`: counts scam/urgency/money/emotion signals, punctuation patterns, and language mix







#### **Feature Engineering** (traditional ML path)

**What it does**: Converts Swahili text messages into numerical data that traditional machine learning algorithms can understand and analyze.

**File**: `src/feature_extractor.py` (class `SwahiliFeatureExtractor`)

**Two main approaches**:

##### **1. Statistical Text Analysis (TF-IDF)**:

* Analyzes how frequently words appear in scam vs. legitimate messages
* Looks at single words (1-grams) and word pairs (2-grams) like "piga simu" or "tuma pesa"
* Identifies the 1000 most important word patterns that help distinguish scams
* **Example**: Words like "hongera" (congratulations) appear more in scam messages

##### **2. Custom Linguistic Features** (Swahili-specific patterns):

* **Phone numbers**: Counts how many phone numbers appear (scams often include contact numbers)
* **Money amounts**: Detects mentions of money like "TSH 1,000,000" or "shilingi elfu"
* **Action words**: Counts urgent commands like "piga" (call), "tuma" (send), "confirm"
* **Congratulatory words**: Detects celebration terms like "hongera", "bahati", "ushindi" (winner)
* **Time pressure words**: Identifies urgency like "sasa" (now), "haraka" (quickly), "urgent"

**Output**: Creates two files with numerical representations:

1. `features.csv`: Each message becomes a row of numbers representing all detected patterns
2. `labels.csv`: Corresponding scam (1) or legitimate (0) labels for training





#### **Modeling** (Teaching the computer to recognize scam patterns)

##### **BiLSTM with Attention** (primary deployed model)

**What it is**: A neural network that reads Swahili text like a human would - from left to right AND right to left simultaneously, paying special attention to important words.

**File**: `src/bilstm_model.py`

**How it processes text**:

* **Word Dictionary Creation**: Builds a vocabulary of all unique Swahili words in the dataset, plus special tokens:
  * `<PAD>`: Fills empty spaces to make all messages the same length
  * `<UNK>`: Represents unknown words not seen during training
  * `<START>` and `<END>`: Mark message boundaries

* **Text to Numbers**: Converts each Swahili word to a unique number the computer can process

**The Neural Network Architecture**:

* **Bidirectional LSTM**: Reads messages in both directions to understand context better
  * **Example**: In "Piga simu sasa", it understands "piga" better by also knowing "sasa" comes later
* **Attention Mechanism**: Focuses on the most important words for making decisions
  * Like highlighting key phrases: "**hongera**", "**tuma pesa**", "**haraka**"
* **Classification Head**: Final layer that outputs scam probability (0-100%)

**Training Process**:

* **Smart data splitting**: Ensures equal representation of scam/legitimate messages in training/testing
* **Learning rate adjustment**: Automatically slows down learning when improvement plateaus
* **Gradient clipping**: Prevents the model from making too-large learning jumps
* **Early stopping**: Saves the best-performing version and stops when no more improvement
* **Model checkpoint**: Saves best model to `models/bilstm_best_model.pth`

**Performance Measurement**: Tests accuracy, precision (how many predicted scams are actually scams), recall (how many actual scams were caught), and F1 (balanced score)





#### **Serving** (Flask API + responsive web UI)

**File**: `api/app.py`

**Model loading**: `load_model()` → `load_bilstm_model()` (default), loads `models/bilstm_best_model.pth` and wraps it with a `predict_proba` interface

**Endpoints**:

* **GET /** → responsive HTML interface (mobile-friendly) for manual testing
* **POST /api/predict** → returns `{is_scam, confidence, scam_probability, safe_probability, model_type}`
* **GET /api/status** → health and model status

**Prediction flow** (/api/predict):

If BiLSTM is loaded (default), text is preprocessed and converted to index sequences; model outputs class probabilities via softmax; confidence is the scam-class probability







### **2. SECTION 6.4 — BACKEND SYSTEM CODE (FLASK API) EXPLANATION**

**Overview**: The entire `api/app.py` file (470 lines) serves as a complete web server that takes Swahili text messages and returns scam predictions. Think of it as a smart assistant that you can ask "Is this message a scam?" and it responds with a yes/no answer plus confidence level.

**File structure**: `api/app.py` contains four main components:

1. **System initialization** (Lines 1-21): Setting up the environment
2. **AI model management** (Lines 22-134): Loading and preparing different AI models
3. **Prediction logic** (Lines 136-192): Converting text to predictions
4. **Web service endpoints** (Lines 194-470): Handling requests from users/applications



#### **A) System Initialization** (Lines 1-21)

**What happens**: The system prepares itself to run

**Technical**: Imports Flask framework, adds `src/` directory to Python path, imports `SwahiliPreprocessor`, i.e. setting up a workspace - gathering all the tools needed before starting work

**Global Variables**: Creates placeholders for the AI model, feature extractor, and text preprocessor that will be used throughout the application



#### **B) AI Model Management** (Lines 22-134)

**What it does**: Decides which AI model to use and loads it into memory

**1. Model Selection Strategy** (Lines 22-30):

* `load_best_model()`: Currently hardcoded to always choose BiLSTM
* **Why BiLSTM**: It's the primary deployed model that performs best for Swahili scam detection

**2. BiLSTM Model Loading** (Lines 61-125):

**File loaded**: `models/bilstm_best_model.pth` (the trained neural network)

**What's inside the file**:

* **Model weights** (the "learned knowledge")
* **Vocabulary dictionary** (all known Swahili words mapped to numbers)

**BiLSTMWrapper Class** (Lines 81-115): Creates a standardized interface

* **Purpose**: Makes the BiLSTM model work the same way as other models
* **Key methods**:
  * `predict_proba()`: Takes text, returns scam probability
  * `text_to_sequence()`: Converts Swahili words to numbers the model understands

**Text Processing Flow**: Raw text → `SwahiliPreprocessor.preprocess()` → word-to-number conversion → neural network → probability

**3. Fallback Options** (Lines 31-59, 127-131):

* **Traditional ML Models**: Random Forest, SVM, Logistic Regression (requires `models/best_model.pkl`)
* **BERT Model**: Google's advanced language model (placeholder for future implementation)
* **Smart Fallback**: If BiLSTM fails to load, automatically tries traditional ML models



**2. Universal Prediction Function** (Lines 170-192):

* `predict_message()`: Works with any loaded model type
* **BiLSTM Path**: Direct text input → preprocessing → neural network → probability
* **Traditional ML Path**: Text → feature extraction → mathematical model → probability
* **Decision Logic**: Scam if confidence > 50% (0.5 threshold)
* **Output**: Returns (`is_scam`: boolean, `confidence`: float)







#### **D) Web Service Endpoints** (Lines 194-470)

**What it provides**: Four web addresses that external applications can access

**1. Main Web Interface - GET /** (Lines 194-363):

**Purpose**: User-friendly webpage for testing the system

**What you see**: Text box to enter Swahili message + "Check Message" button

**Features**:

* **Responsive Design**: Works on desktop, tablet, and mobile phones
* **Real-time Results**: Shows scam/safe classification with confidence percentage
* **Visual Feedback**: Red background for scams, green for safe messages
* **Bilingual**: Interface in both Swahili and English

**Technical Implementation**: Embedded HTML/CSS/JavaScript that calls the prediction API





**2. Prediction API - POST /api/predict** (Lines 365-399):

**Purpose**: The core service that other applications use to get predictions

**Input Format**: JSON `{"message": "Hongera! Umeshinda..."}`

**Processing Flow**:

1. Validates message is provided
2. Checks if AI model is loaded and ready
3. Calls `predict_message()` with the text
4. Formats response with detailed probabilities

**Output Format**:
```json
{
  "message": "original text",
  "is_scam": true/false,
  "confidence": 0.85,
  "scam_probability": 0.85,
  "safe_probability": 0.15,
  "model_type": "Bi-LSTM"
}
```

**Error Handling**: Returns appropriate error codes if something goes wrong





**3. Health Check - GET /api/status** (Lines 401-413):

**Purpose**: Allows monitoring systems to check if the service is working

**Information provided**:

* Service status (running/stopped)
* Whether AI model is loaded successfully
* Which model type is currently active
* System version number

**Use case**: DevOps teams use this to monitor system health in production





**4. Testing Endpoint - GET /api/test** (Lines 415-452):

**Purpose**: Automated testing with predefined Swahili messages

**Test Messages Include**:

* **Scam examples**: "Hongera! Umeshinda TSH 1,000,000..."
* **Legitimate examples**: "Habari za asubuhi. Mkutano wetu..."

**Output**: Batch predictions for all test messages

**Use case**: Developers use this to verify the system works after updates



**Example Usage Scenario**: A mobile app wants to check if a received SMS is a scam

1. **App sends HTTP request**: POST /api/predict with message text
2. **Backend processes**: Text → preprocessing → AI model → probability calculation
3. **Response returned**: JSON with scam classification and confidence score
4. **App displays**: Warning to user if scam probability > 50%

**Real-World Usage Scenarios**:

* **Mobile App Integration**: A Swahili messaging app could integrate this API to warn users about suspicious messages in real-time
* **Bank Security**: Financial institutions could use this to scan customer communications for potential fraud attempts
* **Social Media Monitoring**: Platforms could automatically flag potentially fraudulent content in Swahili
* **Educational Tool**: Schools could use the web interface to teach students about digital literacy and scam recognition

**Design Principles**:

* **Simplicity**: Single prediction function works with any model type
* **Reliability**: Multiple fallback mechanisms ensure service stays online
* **Performance**: Models loaded once at startup, not per request
* **Accessibility**: Web interface works on any device with a browser
* **Extensibility**: Easy to add new model types or modify confidence thresholds



## **3. ANSWERS TO REVIEWER QUESTIONS**

{{ ... }}

> The system provides users with interpretable confidence scores for each prediction.

---

## **6. DETAILED Q&A WITH CODE REFERENCES (FILES • METHODS • LINE NUMBERS)**

## **Q&A Section**

**Q1: Where is the dataset created and what does it contain?**

**A1**: The dataset is created in `data/sample_data.py` (lines 6-1121). The `create_sample_dataset()` function (lines 6-1121) generates synthetic Swahili scam and legitimate messages. It contains:

* **Scam messages**: Fake lottery wins, fraudulent job offers, romance scams
* **Legitimate messages**: Business communications, family messages, news updates
* Each message is labeled as 'scam' or 'legitimate'
* The dataset is saved using `save_dataset()` function (lines 1122-1135)
* Dataset analysis is performed by `analyze_dataset()` function (lines 1137-1153)

**Q2: How are features extracted and saved for machine learning?**

**A2**: Feature extraction happens in `src/feature_extractor.py` using the `SwahiliFeatureExtractor` class (lines 1-184). The process includes:

* **TF-IDF Setup**: Lines 19-35 initialize TF-IDF vectorizer for text analysis
* **Linguistic Features**: Lines 37-89 extract custom features like message length, urgency words, contact patterns
* **Feature Preparation**: Lines 91-140 combine TF-IDF and linguistic features
* **Saving Process**: Lines 142-184 save the processed features and labels to CSV files (`features.csv` and `labels.csv` in `data/processed/`)

**Q3: How does the BiLSTM model work and get trained?**

**A3**: The BiLSTM model is implemented in `src/bilstm_model.py`:

* **Model Definition**: Lines 17-105 define the `BiLSTMClassifier` with embedding, LSTM, and attention layers
* **Dataset Preparation**: Lines 151-200 tokenize text and create training datasets
* **Training Process**: Lines 202-260 implement the training loop with early stopping
* **Evaluation**: Lines 262-304 calculate accuracy, precision, recall, and F1-score metrics
* The model uses attention mechanism to focus on important words in Swahili text

**Q4: How does the BERT model get fine-tuned for Swahili?**

**A4**: BERT fine-tuning is handled in `src/bert_model.py` using the `SwahiliBERTTrainer` class (lines 10-130):

* **Model Initialization**: Lines 15-35 load pre-trained multilingual BERT and tokenizer
* **Dataset Preparation**: Lines 37-65 tokenize Swahili text and prepare training data
* **Training Setup**: Lines 67-95 configure Hugging Face Trainer with learning rate, batch size, epochs
* **Evaluation**: Lines 97-130 use `compute_metrics` function to calculate performance during training
* The model leverages BERT's multilingual capabilities for Swahili text understanding

**Q5: How does the API load and use models for predictions?**

**A5**: The API in `api/app.py` loads models at startup and uses them for predictions:

* **Model Loading**: Lines 22-72 contain `load_bilstm_model()` and `load_traditional_model()` functions
* **BiLSTM Loading**: Creates a wrapper class that exposes `predict_proba()` and `text_to_sequence()` methods
* **Preprocessing**: Uses `SwahiliPreprocessor` for consistent text cleaning between training and serving
* **Prediction Function**: Lines 194-230 contain `predict_message()` that works with any loaded model type
* **Fallback Strategy**: If BiLSTM fails to load, automatically falls back to traditional ML models
* **API Endpoints**: Lines 365-452 provide `/api/predict`, `/api/status`, and `/api/test` endpoints for external access

{{ ... }}

Q10. Where are processed features/labels used by the API's traditional path?



File: api/app.py

Feature construction for single prediction: extract\_features\_for\_prediction(message) at lines 136–168

Loads training columns from data/processed/features.csv at lines 147–163 to align columns before calling model.predict\_proba



Q11. Where can I change max input length for the BiLSTM?



Files: api/app.py and src/bilstm\_model.py

Inference wrapper: BiLSTMWrapper.text\_to\_sequence(..., max\_length=128) at lines 102–113 of api/app.py

Dataset side: SwahiliTextDataset.\_\_init\_\_(..., max\_length=128) at lines 21–26 and padding logic in text\_to\_sequence() at lines 55–58 of src/bilstm\_model.py



Q12. How are evaluation metrics computed and reported?



BiLSTM: evaluate\_model() at lines 275–304 of src/bilstm\_model.py computes Accuracy/Precision/Recall/F1 (weighted)

BERT: compute\_metrics() at lines 66–76 of src/bert\_model.py and printed in train\_model() at lines 113–121



NOTES ON LINE NUMBERS

Line numbers refer to the versions in this repository as of this documentation edit. If you modify files, search by function/class names to locate the exact blocks again.

